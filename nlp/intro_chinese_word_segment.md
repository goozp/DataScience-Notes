# 中文分词

我更多地关注中文分词。

为什么讨论中文分词，分词在英文中很简单，词典分词甚至空格切分就有不错的效果，而中文分词就难得多。中文里不同的切割方式会导致歧义。

中文分词算法分为：
- 词典分词：最简单、最常见的分词算法；如果给定一个词典，词典分词就是一个确定的查词与输出的规则系统。
- 机器学习分词

## 词典分词

我们之前网上找一些公开的词库使用，比如[搜狗实验室互联网词库](http://www.sogou.com/labs/resource/w.php)、[清华大学开放中文词库](http://thuocl.thunlp.org/)、或者一些其它开源词库比如 [fighting41love/funNLP](https://github.com/fighting41love/funNLP)。注意它们的更新时间。

基于词库，我们进行词的切分。

切分算法有：
- 完全切分：实际上以下算法都是基于完全切分过程的。
- 正向最长匹配
- 逆向最长匹配
- 双向最长匹配

双向最长匹配基于正向最长匹配和逆向最长匹配，但是并不能保证一定能选择出最好结果，所有基于规则集的词典分词缺点很明显，没法完美解决某一问题，优时效果可能更差。

这里需要分清一个概念，**词典**和**语料库**，不是一个东西。一般情况下是加载词典构造分词器，然后对语料库进行分词，语料库是训练数据或者测试数据。简单粗暴地理解，可以理解为我们阅读一篇文章（语料库）时，翻字典（词典）查查每一个词是什么意思一样（只不过我们的任务是区分出一个个的词）。

分词后，准确率评测：
- 准确率（accuracy）
- 混淆矩阵 TP/FN/FP/TN
- 精确率
- 召回率
- $F_1$ 值

## 基于统计的机器学习算法分词

例如：
- n-gram 分词
- 隐马尔可夫模型
- 结构化感知机
- ......
